{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iamanantalok/Retail-Sales-Prediction-Rossmann/blob/main/Capstone_2_Retail_Sales_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retail Sales Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name**            - Anant Alok\n",
        "\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the ever-evolving landscape of the retail industry, data-driven decision-making has become paramount for success. Retailers are constantly seeking ways to optimize their operations, enhance customer experiences, and maximize profitability. This project aims to address these challenges by leveraging regression analysis to predict future retail sales.\n",
        "\n",
        "Retail sales prediction is a critical task for businesses in the retail sector. Accurate forecasts enable retailers to make informed decisions regarding inventory management, staffing, marketing strategies, and expansion plans. By harnessing the power of regression analysis, this project seeks to provide retailers with a valuable tool to improve their bottom line.\n",
        "\n",
        "The foundation of any predictive analysis is data. For this project, we collected historical sales data from the retailer, including information on sales volumes, pricing, promotional activities, and external factors like economic indicators and holidays. The dataset spans several years, allowing for a comprehensive analysis.\n",
        "\n",
        "Data preprocessing is a crucial step in any data-driven project. We cleaned the dataset by handling missing values, outliers, and duplicate entries. Feature engineering was performed to create relevant variables, such as lag features to account for seasonality, and dummy variables to encode categorical variables like product categories and store locations.\n",
        "\n",
        "EDA was conducted to gain a deeper understanding of the data. We visualized key trends, patterns, and correlations between variables. EDA revealed insights into sales behavior, seasonality, and the impact of promotions on sales.\n",
        "\n",
        "For this project, we employed multiple regression techniques, including linear regression, decision tree regression, and random forest regression. We assessed the performance of each model using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared to determine the best-fitting model.\n",
        "\n",
        "The dataset was split into training and testing sets to evaluate the model's performance. Cross-validation techniques were employed to avoid overfitting and ensure generalizability. The model's ability to predict future sales was rigorously assessed, and hyperparameters were fine-tuned to optimize performance.\n",
        "\n",
        "To identify the key drivers of retail sales, a feature importance analysis was conducted using the selected regression model. This analysis revealed which factors had the most significant impact on sales, enabling retailers to focus their efforts on these influential variables.\n",
        "\n",
        "Model Deployment:\n",
        "The final regression model was deployed into a user-friendly interface or integrated into the retailer's existing systems for real-time sales prediction. This allows retailers to make data-driven decisions on pricing, inventory management, and marketing strategies.\n",
        "\n",
        "Results and Recommendations:\n",
        "The predictive model achieved a high level of accuracy in forecasting retail sales, with a low Mean Absolute Error and high R-squared value. The feature importance analysis highlighted the importance of factors such as promotions, seasonality, and economic indicators in driving sales. Based on these insights, we recommend the following actions for retailers:\n",
        "\n",
        "Optimize promotional strategies by identifying the most effective types and timing of promotions.\n",
        "Align inventory management with predicted sales to minimize stockouts and overstock situations.\n",
        "Tailor marketing efforts to leverage seasonal trends and external economic conditions.\n",
        "Continuously monitor and update the model to adapt to changing market dynamics.\n",
        "Conclusion:\n",
        "In conclusion, this project demonstrates the power of regression analysis in predicting retail sales. By harnessing historical data and leveraging advanced analytics, retailers can gain a competitive edge in a dynamic industry. Accurate sales forecasts enable retailers to make data-driven decisions that enhance customer satisfaction, increase profitability, and drive business growth. As the retail landscape continues to evolve, predictive analytics will remain an essential tool for success."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rossmann, a retail chain with over 3,000 drug stores across seven European countries, is currently dealing with the issue of predicting their daily sales for up to six weeks ahead. This prediction task is complicated by numerous factors, including promotions, competition, holidays, seasons, and the unique characteristics of each store's location. Store managers face varying challenges in accurately forecasting sales due to these diverse circumstances.**\n",
        "\n",
        "**To address this challenge, historical sales data for 1,115 Rossmann stores has been provided. The goal is to generate sales forecasts for the \"Sales\" column in the test dataset, while accounting for the fact that some stores in the dataset were temporarily closed for renovations.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# Import numpy for numerical computations and array manipulation\n",
        "import numpy as np\n",
        "\n",
        "# Import pandas for data manipulation and analysis with DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "# Import plotly express for interactive data visualization\n",
        "import plotly.express as px\n",
        "\n",
        "# Import matplotlib.pyplot for static plotting and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import seaborn for higher-level statistical visualizations\n",
        "import seaborn as sns\n",
        "\n",
        "# Import datetime for working with date and time data\n",
        "from datetime import datetime\n",
        "\n",
        "# Import warnings to handle and filter warning messages\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import scipy.stats for statistical functions and probability distributions\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Import SelectKBest, f_regression for feature selection based on statistical tests\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "# Import StandardScaler for feature scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Import train_test_split for splitting data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import regression models: LinearRegression, Ridge, Lasso\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "\n",
        "# Import regression metrics: r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse, mean_absolute_error as mae\n",
        "\n",
        "# Import math module for basic mathematical operations\n",
        "import math\n",
        "\n",
        "# Import GridSearchCV, RandomizedSearchCV for hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Import xgboost for gradient boosting algorithms\n",
        "import xgboost as xgb\n",
        "\n",
        "# Import XGBRegressor, a scikit-learn compatible wrapper for XGBoost's regression model\n",
        "from xgboost.sklearn import XGBRegressor\n",
        "\n",
        "# Import DecisionTreeRegressor for regression based on decision trees\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Import RandomForestRegressor for regression based on random forests\n",
        "from sklearn.ensemble import RandomForestRegressor\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset(Mounting google drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Wb_yd8HgPrHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "path = \"/content/drive/MyDrive/Capstone-project-2-Retail-Sales-Prediction/\"\n",
        "rossmann_sales = pd.read_csv(path + \"Rossmann Stores Data.csv\", low_memory=False)\n",
        "stores = pd.read_csv(path + \"store.csv\")"
      ],
      "metadata": {
        "id": "iWr_K2X1P3oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Displays all dataframe columns\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "# Creating a copy of the original dataframe 'rossmann_sales' , 'stores' and assigns it to the variables 'rossmann_sales_df' and 'stores_df'.\n",
        "rossmann_sales_df = rossmann_sales.copy()\n",
        "stores_df = stores.copy()\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing first 5 rows and last 5 rows of rossmann_sales_df dataframe\n",
        "rossmann_sales_df.head().append(rossmann_sales_df.tail())"
      ],
      "metadata": {
        "id": "ecu9Vpx_TgeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing first 5 rows and last 5 rows of stores_df dataframe\n",
        "stores_df.head().append(stores_df.tail())"
      ],
      "metadata": {
        "id": "PGP2GtGbT-S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'Rossmann sales dataset has {rossmann_sales_df.shape} rows and columns respectively.')\n",
        "print(f'Stores dataset has {rossmann_sales_df.shape} rows and columns respectively.')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Rossmann Sales Dataset Info\n",
        "rossmann_sales_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stores dataset info\n",
        "stores_df.info()"
      ],
      "metadata": {
        "id": "d7MuJdMEWii6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count duplicate values in rossmann_sales_df\n",
        "print(f\"Duplicate count in Rossmann Sales DataFrame: {rossmann_sales_df.duplicated().sum()}\")\n",
        "\n",
        "# Count duplicate values in stores_df\n",
        "print(f\"Duplicate count in Stores DataFrame: {stores_df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values count in rossmann_sales_df\n",
        "sales_misssing_count = rossmann_sales_df.isnull().sum()\n",
        "\n",
        "# Missing values count in stores_df\n",
        "stores_missing_count = stores_df.isnull().sum()\n",
        "\n",
        "# Printing count of missing values for both dataframes\n",
        "print(\"Null Value Counts in Rossmann Sales DataFrame:\\n\",sales_misssing_count)\n",
        "print(\"\\nNull Value Counts in Stores DataFrame:\\n\",stores_missing_count)\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "# Calculate the percentage of missing values in each column for stores_df\n",
        "stores_missing_value_percent = (stores_df.isnull().sum() / len(stores_df)) * 100\n",
        "\n",
        "# Create a bar plot to visualize the missing values\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=stores_missing_value_percent.index, y=stores_missing_value_percent.values,palette='coolwarm')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('% of Missing Values')\n",
        "plt.title('Percentage of Missing Values in Stores DataFrame')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CompetitionDistance:** There are only 3 missing values for the CompetitionDistance column, suggesting that the majority of stores have recorded information about their nearest competitor's distance.\n",
        "\n",
        "**CompetitionOpenSinceMonth** and **CompetitionOpenSinceYear:** Both of these columns have 354 missing values each, indicating that a considerable number of stores lack information regarding the month and year when their nearest competitor opened.\n",
        "\n",
        "**Promo2SinceWeek** and **Promo2SinceYear:** Similar to CompetitionOpenSinceMonth and CompetitionOpenSinceYear, these columns also have 544 missing values each. This implies that a significant portion of stores does not have data on when they initiated their participation in Promo2, which is an ongoing and consecutive promotion.\n",
        "\n",
        "**PromoInterval:** The PromoInterval column also contains 544 missing values, suggesting that many stores do not possess information about the specific intervals at which they participate in Promo2.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**We are now focusing on addressing the issue of missing values within the stores dataset.**\n",
        "\n",
        "1. The \"**CompetitionDistance**\" variable represents the distance in meters to the nearest competitor store. Analyzing the distribution plot of these distances will provide insights into the typical opening distances for stores, helping us decide how to fill in missing values for this variable."
      ],
      "metadata": {
        "id": "QX-zr30Hn2Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution plot of competition distance\n",
        "\n",
        "# Create a distribution plot\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.distplot(stores_df['CompetitionDistance'], hist=True, color=\"skyblue\", bins=30)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Competition Distance (meters)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Competition Distances Among Stores')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wllpwv6IoMZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the CompetitionDistance variable indicates a left-skewed pattern, with the majority of values clustered towards the lower end. In such cases, using the median as a measure of central tendency is a more robust choice because it is less influenced by outlier values."
      ],
      "metadata": {
        "id": "QZxV9KzApDXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# filling competition distance with the median value\n",
        "stores_df['CompetitionDistance'].fillna(stores_df['CompetitionDistance'].median(), inplace=True)\n",
        "\n",
        "# Convert the column to 'int64'\n",
        "stores_df['CompetitionDistance'] = stores_df['CompetitionDistance'].astype('int64')"
      ],
      "metadata": {
        "id": "f-FmSGEipV9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Filling missing values in the **'CompetitionOpenSinceMonth'** and **'CompetitionOpenSinceYear'** columns with their respective modes is appropriate because it captures the most common values, preserving the typical patterns for competition opening dates across stores. This approach maintains the central tendency of the data, ensures data integrity, and is simple to understand. However, the choice of imputation method should always consider the dataset's specific context and problem at hand, as alternative methods may be more suitable in certain scenarios."
      ],
      "metadata": {
        "id": "cXpx8l6lrp_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with the mode (most frequent value) of each respective column\n",
        "\n",
        "stores_df['CompetitionOpenSinceMonth'].fillna(stores_df['CompetitionOpenSinceMonth'].mode()[0], inplace=True)\n",
        "stores_df['CompetitionOpenSinceYear'].fillna(stores_df['CompetitionOpenSinceYear'].mode()[0], inplace=True)\n"
      ],
      "metadata": {
        "id": "dBRzyhJdrJ9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.\n",
        "Filling NaN values in the **'Promo2'** related columns with 0 is a reasonable choice due to the binary nature of these columns, where 1 signifies the presence of a promotion, and 0 indicates its absence. Imputing with 0 is a consistent and easily interpretable approach, maintaining dataset integrity."
      ],
      "metadata": {
        "id": "q3f2LUXIsZE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute NaN values in Promo2 related columns with 0\n",
        "columns_to_impute = ['Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']\n",
        "stores_df[columns_to_impute] = stores_df[columns_to_impute].fillna(0)\n"
      ],
      "metadata": {
        "id": "aB2WLNTXsAn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values and display the count for each column\n",
        "missing_values = stores_df.isnull().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "PmuFaG_1swma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "# Columns of rossmann sales dataframe\n",
        "print('Columns of Rossmann Sales Dataset are:\\n',rossmann_sales_df.columns)\n",
        "\n",
        "# Columns of stores dataframe\n",
        "print('Columns of Stores Dataset are:\\n',stores_df.columns)"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "# Discribe rossmann dataset\n",
        "print('Discription of Rossmann Dataset:\\n',rossmann_sales_df.describe().T)\n",
        "\n",
        "# Printing seperation line between two dataset\n",
        "print('-'*100)\n",
        "\n",
        "#Discribe stores dataset\n",
        "print('Discription of Stores Dataset:\\n',stores_df.describe().T)"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rossmann Stores Data.csv** - historical data including Sales\n",
        "\n",
        "**store.csv** - supplemental information about the stores\n",
        "\n",
        "**Most of the fields are self-explanatory.**\n",
        "\n",
        "**1.Id** - an Id that represents a (Store, Date) duple within the set.\n",
        "\n",
        "**2.Store** - a unique Id for each store.\n",
        "\n",
        "**3.Sales** - the turnover for any given day (Dependent Variable).\n",
        "\n",
        "**4.Customers** - the number of customers on a given day.\n",
        "\n",
        "**5.Open** - an indicator for whether the store was open: 0 = closed, 1 = open.\n",
        "\n",
        "**6.StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None.\n",
        "\n",
        "**7.SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools.\n",
        "\n",
        "**8.StoreType** - differentiates between 4 different store models: a, b, c, d.\n",
        "\n",
        "**9.Assortment** - describes an assortment level: a = basic, b = extra, c = extended. An assortment strategy in retailing involves the number and type of products that stores display for purchase by consumers.\n",
        "\n",
        "**10.CompetitionDistance** - distance in meters to the nearest competitor store.\n",
        "\n",
        "**11.CompetitionOpenSince**[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened.\n",
        "\n",
        "**12.Promo** - indicates whether a store is running a promo on that day.\n",
        "\n",
        "**13.Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating.\n",
        "\n",
        "**14.Promo2Since**[Year/Week] - describes the year and calendar week when the store started participating in Promo2.\n",
        "\n",
        "**15.PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique Values for each variable in rossmann_sales_df (excluding 'Date')\n",
        "for column in rossmann_sales_df.columns:\n",
        "    if column != 'Date':\n",
        "        unique_values = rossmann_sales_df[column].unique()\n",
        "        print(f\"Unique Values for {column}:\\n{unique_values}\\n\")"
      ],
      "metadata": {
        "id": "OIkUSqto7FGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the count of unique values for each variable (excluding 'Date')\n",
        "for column in rossmann_sales_df.columns:\n",
        "    if column != 'Date':\n",
        "        unique_values_count = rossmann_sales_df[column].nunique()\n",
        "        print(f\"Unique Value Count for {column}: {unique_values_count}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique Values for each variable in stores_df\n",
        "for column in stores_df.columns:\n",
        "    unique_values = stores_df[column].unique()\n",
        "    print(f\"Unique Values for {column}:\\n{unique_values}\\n\")"
      ],
      "metadata": {
        "id": "X6cc_a3S7P6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the count of unique values for each variable (excluding 'Date')\n",
        "for column in stores_df.columns:\n",
        "    if column != 'Date':\n",
        "        unique_values_count = stores_df[column].nunique()\n",
        "        print(f\"Unique Value Count for {column}: {unique_values_count}\")"
      ],
      "metadata": {
        "id": "XnquyRoY7S1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert \"Date\" column to datetime datatype\n",
        "rossmann_sales_df['Date'] = pd.to_datetime(rossmann_sales_df['Date'])\n",
        "\n",
        "# Extract year, month, and day of the week\n",
        "rossmann_sales_df['Year'] = rossmann_sales_df['Date'].dt.year\n",
        "rossmann_sales_df['Month'] = rossmann_sales_df['Date'].dt.month\n",
        "rossmann_sales_df['DayOfMonth'] = rossmann_sales_df['Date'].dt.day\n",
        "rossmann_sales_df['WeekOfYear'] = rossmann_sales_df['Date'].dt.weekofyear\n",
        "\n",
        "# Display the updated Rossman Sales DataFrame with added date-related columns\n",
        "print(\"Rossman Sales DataFrame with Date Information:\")\n",
        "rossmann_sales_df.head()\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count how many rows have 0 sales value\n",
        "num_rows_with_zero_sales = len(rossmann_sales_df[rossmann_sales_df['Sales'] == 0])\n",
        "print(\"Number of rows with 0 sales value:\", num_rows_with_zero_sales)\n",
        "\n",
        "# Count how many rows have 0 customer value\n",
        "num_rows_with_zero_customers = len(rossmann_sales_df[rossmann_sales_df['Customers'] == 0])\n",
        "print(\"Number of rows with 0 customer value:\", num_rows_with_zero_customers)\n",
        "\n",
        "# Display updated Rossman Sales DataFrame after data cleaning\n",
        "print(\"Rossman Sales DataFrame after removing rows with 0 sales and 0 customers:\")\n",
        "rossmann_sales_df.head()\n"
      ],
      "metadata": {
        "id": "Ecv9W8Wt9Tl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map 'StateHoliday' values to a binary format (1 for holidays, 0 for non-holidays)\n",
        "rossmann_sales_df['StateHoliday'] = rossmann_sales_df['StateHoliday'].replace({'a': 1, 'b': 1, 'c': 1, '0': 0})\n",
        "\n",
        "# Display updated Rossman Sales DataFrame with 'StateHoliday' converted to binary\n",
        "print(\"Rossman Sales DataFrame with 'StateHoliday' converted to binary:\")\n",
        "rossmann_sales_df.head()\n"
      ],
      "metadata": {
        "id": "P2utfm5J9r3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Type Conversion for Specific Columns\n",
        "stores_df['CompetitionOpenSinceMonth'] = stores_df['CompetitionOpenSinceMonth'].astype('Int64')\n",
        "stores_df['CompetitionOpenSinceYear'] = stores_df['CompetitionOpenSinceYear'].astype('Int64')\n",
        "stores_df['Promo2SinceWeek'] = stores_df['Promo2SinceWeek'].astype('Int64')\n",
        "stores_df['Promo2SinceYear'] = stores_df['Promo2SinceYear'].astype('Int64')\n"
      ],
      "metadata": {
        "id": "W8Zd2Soe-B4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Filtering: Remove rows with 'CompetitionOpenSinceYear' values 1900 and 1961\n",
        "stores_df = stores_df[~stores_df['CompetitionOpenSinceYear'].isin([1900, 1961])]\n",
        "\n",
        "# Reset Index: Reorganize the DataFrame index after filtering\n",
        "stores_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Display the updated Rossman Stores DataFrame after data filtering and index reset\n",
        "print(\"Rossman Stores DataFrame after filtering and index reset:\")\n",
        "stores_df.head()\n"
      ],
      "metadata": {
        "id": "3d5Xck9n-P-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Merging: Merge the datasets on the 'Store' column using an inner join\n",
        "merged_df = pd.merge(rossmann_sales_df, stores_df, on='Store', how='inner')\n",
        "\n",
        "# Display the merged DataFrame containing sales and store information\n",
        "print(\"Merged DataFrame for EDA:\")\n",
        "merged_df.head()\n"
      ],
      "metadata": {
        "id": "vdI7ASs4-f9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sales vs. Competition Distance\n",
        "sales_competition_data = merged_df[['Sales', 'CompetitionDistance']]\n",
        "\n",
        "# Print the extracted data to inspect it\n",
        "print(sales_competition_data)\n",
        "\n",
        "# Calculate the correlation coefficient between Sales and CompetitionDistance\n",
        "correlation_coefficient = sales_competition_data['Sales'].corr(sales_competition_data['CompetitionDistance'])\n",
        "\n",
        "# Print the correlation coefficient\n",
        "print(\"Correlation Coefficient between Sales and CompetitionDistance is:\", correlation_coefficient)\n"
      ],
      "metadata": {
        "id": "zccEahugxNPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by 'Year' and calculate the mean sales for each year\n",
        "sales_by_year = merged_df.groupby('Year')['Sales'].mean().reset_index()\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "sales_by_year\n"
      ],
      "metadata": {
        "id": "ILQpkD_8zwA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Date- Change datatype from object to datetime and extract year, month and days of month.\n",
        "\n",
        "2. Sales & Customers- identify those rows that zero sale value or zero customers.\n",
        "\n",
        "3. StateHoliday- The feature StateHoliday changed into a boolean variable. The value {a, b, c} became 1, other 0.The purpose of this action is to transform categorical values ('a', 'b', 'c', and '0') into their numerical counterparts (1 and 0). This conversion facilitates the utilization of these variables in numerical calculations and analytical processes.\n",
        "\n",
        "4. Converting Data Types: The columns CompetitionOpenSinceMonth, CompetitionOpenSinceYear, Promo2SinceWeek, and Promo2SinceYear are currently in float format. To align them with their nature as representations of months and years, we need to convert them into integer data types, as these values should be whole numbers.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sales Distribution**"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a larger figure size for better visualization\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Create a histogram with a kernel density estimate (KDE) overlay\n",
        "sns.histplot(merged_df['Sales'], kde=True, color='blue')\n",
        "\n",
        "# Add a clear and descriptive title\n",
        "plt.title('Distribution of Sales in Rossmann Stores', fontsize=16, fontweight='bold', color='navy')\n",
        "\n",
        "# Label the x and y axes\n",
        "plt.xlabel('Sales', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "\n",
        "# Display a legend\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose to create a histogram with a kernel density estimate (KDE) plot for visualizing the distribution of the 'Sales' variable because it is a suitable choice for several reasons. Firstly, it effectively displays how sales values are distributed across different ranges, providing insight into their frequency distribution. Secondly, as 'Sales' is a continuous numerical variable, a histogram is an appropriate visualization method for such data, allowing us to see the distribution's shape and range. Additionally, it helps identify any skewness or outliers in the data, which can be crucial for understanding sales patterns. Lastly, it enables the visualization of the central tendency of sales, which can be further validated with statistical measures like mean and median."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. The histogram analysis reveals a positively skewed distribution for 'Sales,' indicating that there are more occurrences of lower sales values and fewer instances of exceptionally high sales.\n",
        "2. The central tendency of the data, represented by the peak of the histogram, is observed to be in the range of 6,000 to 8,000 sales, indicating where the most common sales values are concentrated.\n",
        "3. The width of the distribution illustrates the spread and variability in the sales data, with a moderate spread covering a range from approximately 0 to 40,000 sales values.\n",
        "4. Notably, there are potential outliers in the data, visible as sales values that significantly deviate from the central region and extend to the right in the histogram's long tail. These outliers represent unusually high sales values compared to the majority of the data points.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "Gaining Insights into Sales Distribution: The histogram and KDE plot provide valuable insights into the distribution of sales, enabling businesses to identify common sales figures and assess the overall sales spread. This understanding aids in setting realistic sales targets, optimizing inventory management, and allocating resources effectively.\n",
        "\n",
        "Negative Business Impact:\n",
        "\n",
        "Skewed Distribution Challenges: A heavily skewed sales distribution, primarily concentrated at lower sales values, may pose challenges for achieving significant sales growth or expanding market share. In response, businesses may need to devise strategies to stimulate demand and attract a larger customer base."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Customer Engagement and Spending"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# Overall Title\n",
        "plt.suptitle('Exploring Customer Engagement and Spending Patterns', fontsize=16, fontweight='bold', color='navy')\n",
        "\n",
        "\n",
        "# Average Number of Visits per Customer\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(merged_df.groupby('Customers')['Date'].count(), kde=True, color='purple')\n",
        "plt.title('Distribution of Number of Visits per Customer')\n",
        "plt.xlabel('Number of Visits')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "# Average Spending per Customer\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(merged_df.groupby('Customers')['Sales'].mean(), kde=True, color='orange')\n",
        "plt.title('Distribution of Average Spending per Customer')\n",
        "plt.xlabel('Average Spending')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While histograms, often created using Seaborn's histplot, are a popular choice for visualizing the distribution of single variables like sales or customer visits, they are not necessary for calculating metrics such as visits per customer, average spending per customer, and customer retention rate."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average Number of Visits per Customer: On average, each customer visits the store about 206 times in the dataset's timeframe. This tells us how often customers engage with the store.\n",
        "\n",
        "Average Spending per Customer: On average, each customer spends roughly 13,917 units of currency during their visits. This reveals customer spending habits and overall sales potential."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Number of Visits per Customer:\n",
        "\n",
        "Positive Impact: High visit frequency suggests customer loyalty and satisfaction, potentially boosting customer lifetime value and positive word-of-mouth.\n",
        "\n",
        "Negative Impact: Low visit frequency may indicate customer dissatisfaction, reducing retention and damaging the business's reputation.\n",
        "\n",
        "Average Spending per Customer:\n",
        "\n",
        "Positive Impact: High spending per customer increases revenue and profitability, making these customers more valuable to the business.\n",
        "\n",
        "Negative Impact: Low spending per customer results in reduced revenue and profitability."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sales and Customer Traffic**"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots with two side-by-side axes\n",
        "fig, (axis1, axis2) = plt.subplots(1, 2, figsize=(20, 4))\n",
        "\n",
        "# Promo vs. Sales\n",
        "sns.barplot(x='Promo', y='Sales', data=merged_df, ax=axis1)\n",
        "axis1.set_title('Impact of Promo on Sales')\n",
        "axis1.set_xlabel('Promo')\n",
        "axis1.set_ylabel('Sales')\n",
        "\n",
        "# Promo vs. Customers\n",
        "sns.barplot(x='Promo', y='Customers', data=merged_df, ax=axis2)\n",
        "axis2.set_title('Impact of Promo on Customer Traffic')\n",
        "axis2.set_xlabel('Promo')\n",
        "axis2.set_ylabel('Customers')\n",
        "\n",
        "# Overall Title\n",
        "plt.suptitle('Impact of Promo on Sales and Customer Traffic', fontsize=16, fontweight='bold', color='navy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reason for selecting the chart, which in this case is the grouped bar plot comparing sales and customer traffic during promotional and non-promotional periods, is because it effectively illustrates the impact of promotions on both sales and customer engagement in a straightforward and visually informative manner."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe a substantial increase in both sales and customer traffic during promotional periods. This indicates that promotions have a positive impact on store performance."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights that promotions lead to higher sales and increased customer traffic can have a positive business impact. They enable businesses to boost revenue, engage customers more effectively, make data-driven decisions for marketing, optimize inventory management, gain a competitive edge, and foster customer loyalty."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Average Sales and Sales Growth Trends Over Time (Year-Week)**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by date and get average sales and percent change\n",
        "average_sales = merged_df.groupby('WeekOfYear')[\"Sales\"].mean()\n",
        "pct_change_sales = merged_df.groupby('WeekOfYear')[\"Sales\"].sum().pct_change()\n",
        "\n",
        "# Create subplots\n",
        "fig, (axis1, axis2) = plt.subplots(2, 1, sharex=True, figsize=(15, 8))\n",
        "\n",
        "# Plot average sales over time (year-week)\n",
        "ax1 = average_sales.plot(legend=True, ax=axis1, marker='o', title=\"Average Sales Per Week.\")\n",
        "ax1.set_xticks(range(len(average_sales)))\n",
        "ax1.set_xticklabels(average_sales.index.tolist())\n",
        "ax1.set_ylabel('Sales', size=12)\n",
        "\n",
        "# Plot percent change for sales over time (year-week)\n",
        "ax2 = pct_change_sales.plot(legend=True, ax=axis2, marker='o', colormap=\"summer\", title=\"Sales Percent Change Per Week.\")\n",
        "ax2.set_xlabel('Week Of Year', size=12)\n",
        "plt.ylabel(\"Sales\", size=12)\n",
        "\n",
        "# Overall Title\n",
        "plt.suptitle('Trends and Growth Insights', fontsize=16, fontweight='bold', color='navy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart was selected for visualizing average sales and sales growth over time (Year-Week) due to its ability to effectively display trends, patterns, and comparisons in time-series data. This choice offers clarity, simplicity, and a straightforward representation of how sales metrics change over weeks, making it accessible to a broad audience."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It helps identify sales patterns, seasonality, anomalies, and growth trends. These insights guide inventory management and marketing decisions, improving retail operations. Upon closer examination, it becomes evident that there are fluctuations in weekly sales, with some weeks performing exceptionally well while others experience a decline. Towards the end of the year, particularly in the final few weeks, there is a notable surge in sales."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights gained from these charts can be valuable for making informed decisions in retail, leading to a positive business impact by optimizing operations, marketing efforts, and resource allocation. However, it's essential to analyze the underlying reasons for negative growth and take appropriate actions to mitigate any adverse effects."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sales Analysis**\n"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a grid of subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(8, 6))\n",
        "\n",
        "#Sales vs. StateHoliday\n",
        "sns.barplot(data=merged_df, x='StateHoliday', y='Sales', palette='pastel', ci=None, ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Sales vs. StateHoliday')\n",
        "axes[0, 0].set_xlabel('StateHoliday')\n",
        "axes[0, 0].set_ylabel('Sales')\n",
        "\n",
        "#Sales vs. SchoolHoliday\n",
        "sns.barplot(data=merged_df, x='SchoolHoliday', y='Sales', palette='Set2', ci=None, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Sales vs. SchoolHoliday')\n",
        "axes[0, 1].set_xlabel('SchoolHoliday')\n",
        "axes[0, 1].set_ylabel('Sales')\n",
        "\n",
        "#Sales vs. StoreType\n",
        "sns.barplot(data=merged_df, x='StoreType', y='Sales', palette='tab10', ci=None, ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Sales vs. StoreType')\n",
        "axes[1, 0].set_xlabel('StoreType')\n",
        "axes[1, 0].set_ylabel('Sales')\n",
        "\n",
        "#Sales vs. Assortment\n",
        "sns.barplot(data=merged_df, x='Assortment', y='Sales', palette='Set1', ci=None, ax=axes[1, 1])\n",
        "axes[1, 1].set_title('Sales vs. Assortment')\n",
        "axes[1, 1].set_xlabel('Assortment')\n",
        "axes[1, 1].set_ylabel('Sales')\n",
        "\n",
        "# Add a bit more space between subplots\n",
        "plt.tight_layout(pad=3)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The bar plots show the average sales for each category of the respective variable, aiding the comparison of sales across groups. The color and style choices enhance visualization and category differentiation. Bar plots are apt for this categorical variable analysis, offering insights into how sales are impacted by these factors."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StateHoliday: Stores have lower sales (258.64) on State Holidays (StateHoliday = 1.0) compared to higher sales (5945.92) on non-holidays (StateHoliday = 0.0).\n",
        "\n",
        "SchoolHoliday: Stores experience higher sales (6474.89) on School Holidays (SchoolHoliday = 1.0) compared to lower sales (5619.54) on non-holidays (SchoolHoliday = 0.0).\n",
        "\n",
        "StoreType: StoreType b has the highest sales (10058.84) among all types, followed by StoreType a (5736.60) and StoreType c (5723.63). StoreType d has sales averaging 5639.35.\n",
        "\n",
        "Assortment: Stores with Assortment type b have the highest sales (8553.93) compared to type a (5479.04) and type c (6057.87)."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The insights from the chart offer opportunities for positive business impact:\n",
        "\n",
        "StateHoliday: Lower sales on State Holidays suggest a chance to boost revenue through special promotions and discounts on those days, attracting more customers and increasing sales.\n",
        "\n",
        "SchoolHoliday: Higher sales during School Holidays indicate a potential market. Targeted marketing and optimized inventory can tap into this demand.\n",
        "\n",
        "StoreType: Variations in sales by StoreType highlight successful formats (e.g., StoreType b). Businesses can replicate winning strategies across stores.\n",
        "\n",
        "Assortment: Stores with Assortment type b have the highest sales, signaling customer preferences. Adapting product offerings to match this assortment can enhance sales."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Average number of Customers**"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the DataFrame by 'DayOfWeek' and calculate the mean of 'Customers' for each group\n",
        "average_customers_per_day = merged_df.groupby('DayOfWeek')[['Customers']].mean()\n",
        "\n",
        "# Create a plot with a specified figure size, marker style, and color\n",
        "axis = average_customers_per_day.plot(figsize=(10, 5), marker='^', color='b')\n",
        "\n",
        "# Set the title for the plot\n",
        "axis.set_title('Average Number of Customers per Day of the Week')\n",
        "\n",
        "# Label the x-axis\n",
        "axis.set_xlabel('Day of the Week')\n",
        "\n",
        "# Label the y-axis\n",
        "axis.set_ylabel('Average Number of Customers')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose the line plot because it's a useful method for illustrating trends or fluctuations in a numerical metric, such as the average number of customers, across a continuous variable like the days of the week. The '^' markers represent individual data points for each day, and the connecting line visually displays the overall pattern. The use of the color blue enhances the chart's aesthetics and readability."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The examination shows that Mondays typically experience the highest number of visitors, potentially because it marks the beginning of the workweek, and customers may be more inclined to make purchases after the weekend. In contrast, Sundays witness notably lower foot traffic, possibly because many businesses are closed or have reduced operating hours on Sundays. These findings can inform decisions about staffing and promotional tactics for different days of the week to accommodate the fluctuating patterns of customer demand."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Positive Impact:\n",
        "\n",
        "Efficient Staffing: Optimize staff levels for peak and off-peak days.\n",
        "\n",
        "Tailored Promotions: Customize promotions to boost traffic on slower days.\n",
        "\n",
        "Resource Efficiency: Allocate resources effectively for maximum returns.\n",
        "\n",
        "Negative Insights:\n",
        "\n",
        "Sunday Footfall Drop: Consider viability and attracting more customers on Sundays.\n",
        "\n",
        "Weekday Decline: Analyze and address the gradual weekday footfall decrease."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Average Number of Customers on State Holidays and School Holidays**"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Violin plot for customers vs. SchoolHoliday\n",
        "sns.violinplot(data=merged_df, x='SchoolHoliday', y='Customers', ax=axes[0])\n",
        "axes[0].set_title('Customers vs. SchoolHoliday')\n",
        "axes[0].set_xlabel('SchoolHoliday')\n",
        "axes[0].set_ylabel('Number of Customers')\n",
        "\n",
        "# Violin plot for customers vs. StateHoliday\n",
        "sns.violinplot(data=merged_df, x='StateHoliday', y='Customers', ax=axes[1])\n",
        "axes[1].set_title('Customers vs. StateHoliday')\n",
        "axes[1].set_xlabel('StateHoliday')\n",
        "axes[1].set_ylabel('Number of Customers')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Violin plots are valuable for visualizing how a numerical variable is distributed within categorical groups. They offer a compact summary of data distributions, facilitating straightforward group comparisons. Furthermore, the incorporation of color palettes in violin plots enhances their visual appeal and assists in emphasizing distinctions among categories."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "On State Holidays, the average number of customers drops significantly to 40.13, showcasing a notable decrease in foot traffic compared to regular days. Conversely, on standard non-holiday days, customer attendance rises to an average of 651.84, indicating higher visitation rates. During School Holidays, there is a slight uptick in customer numbers, averaging 704.44, compared to the regular days, where the average customer count slightly dips to 617.67.\n",
        "\n",
        "In summary, these insights emphasize that State Holidays have a more substantial impact on reducing customer footfall compared to School Holidays, with regular non-holiday days drawing the highest number of customers. This information can be instrumental for businesses in tailoring their staffing, inventory management, and promotional strategies around holidays to better cater to customer needs and maximize sales potential."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "Efficient Staffing: Adjust staffing to minimize costs on State Holidays and meet demand during School Holidays.\n",
        "\n",
        "Tailored Promotions: Customize offers for State Holidays to attract shoppers and focus on upselling during School Holidays.\n",
        "\n",
        "Inventory Management: Optimize stock levels for cost savings on State Holidays and ensure adequate supply during School Holidays.\n",
        "\n",
        "These strategies enhance operational efficiency and can boost sales."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Total Customers in Store Type**"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a subplot with 1 row and 2 columns, specifying the figure size\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n",
        "\n",
        "# Plot 1: Share of Store Types\n",
        "store_type_counts = merged_df[\"StoreType\"].value_counts()\n",
        "axes[0].pie(store_type_counts, labels=store_type_counts.index, autopct='%1.1f%%')\n",
        "axes[0].set_title('Share of Store Types')  # Set the title for the first pie chart\n",
        "\n",
        "# Plot 2: Customer Share by Store Type\n",
        "customer_by_store_type = merged_df.groupby('StoreType')['Customers'].sum()\n",
        "axes[1].pie(customer_by_store_type, labels=customer_by_store_type.index, autopct='%1.1f%%')\n",
        "axes[1].set_title('Customer Share by Store Type')  # Set the title for the second pie chart\n",
        "\n",
        "# Adjust the layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pie charts are great for depicting parts-to-whole relationships and are especially beneficial for highlighting the contribution of separate categories to a total or comparing the relative sizes of different categories. They are visually pleasing and provide an easy-to-understand representation of facts."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store Type 'a' holds the largest portion of customers, making up the majority of the total customer base and also having the highest share in terms of numbers."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Impact:\n",
        "- Understanding customer distribution by store type helps tailor marketing efforts, enhancing customer loyalty, especially for Store Type 'a.'\n",
        "- Identifying Store Type 'd' as popular allows replicating successful strategies for overall growth.\n",
        "\n",
        "Negative Growth:\n",
        "- Store Type 'b' has low customer (4.89%) and store type (1.56%) shares, indicating underperformance.\n",
        "- Store Type 'c' performs moderately (14.33% customers, 13.48% store types), prompting competitive analysis for improvement."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **The Impact of Competition Distance on Sales**"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot to visualize the relationship between Sales and CompetitionDistance\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.scatterplot(data=sales_competition_data, x='CompetitionDistance', y='Sales')\n",
        "\n",
        "# Label the x-axis and y-axis\n",
        "plt.xlabel('Competition Distance')\n",
        "plt.ylabel('Sales')\n",
        "\n",
        "# Set the title for the plot and include the correlation coefficient\n",
        "plt.title(f'Correlation between Sales and Competition Distance\\nCorrelation Coefficient: {correlation_coefficient:.2f}')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The scatter plot serves as a tool to help us visually understand how the data points are distributed and whether there might be a connection between sales and competition distance. Meanwhile, the correlation coefficient provides us with a numeric way to gauge both the strength and direction of this relationship. If the correlation coefficient is positive, it indicates a positive connection, whereas a negative coefficient suggests a negative relationship. On the other hand, a value close to zero implies a weak or negligible correlation."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A correlation coefficient of -0.0189 suggests that there is a weak or almost non-existent linear connection between sales and competition distance within the dataset. From a business perspective, this implies that competition distance by itself is unlikely to be a strong indicator of sales performance, and there are likely other factors that exert a more substantial influence on retail store sales."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The modest correlation between sales and competition distance underscores the need for a more comprehensive approach to enhancing sales performance. While competition distance is just one of several factors that can affect sales, depending solely on this factor may not yield substantial business improvements. Instead, companies should embrace a holistic strategy that emphasizes understanding customer preferences, leveraging competitive advantages, and addressing store-specific variables to achieve meaningful growth."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sales Variations : Daily Trends within a Month and Monthly Trends within a Year**"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subplots with 1 row and 2 columns, specifying the figure size\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot 1: Sales vs. Day of Month\n",
        "sns.lineplot(data=merged_df, x='DayOfMonth', y='Sales', ax=axes[0])\n",
        "\n",
        "# Set title and labels for the first subplot\n",
        "axes[0].set_title('Sales vs. Day of Month')\n",
        "axes[0].set_xlabel('Day of Month')\n",
        "axes[0].set_ylabel('Sales')\n",
        "axes[0].grid(True)  # Add grid lines\n",
        "\n",
        "# Plot 2: Sales vs. Month\n",
        "sns.lineplot(data=merged_df, x='Month', y='Sales', ax=axes[1])\n",
        "\n",
        "# Set title and labels for the second subplot\n",
        "axes[1].set_title('Sales vs. Month')\n",
        "axes[1].set_xlabel('Month')\n",
        "axes[1].set_ylabel('Sales')\n",
        "axes[1].grid(True)  # Add grid lines\n",
        "\n",
        "# Adjust the layout for better spacing\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the subplots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When examining the relationship between Sales and Day of Month, a line plot is useful for revealing any notable sales patterns or fluctuations throughout the month. This enables us to detect potential high or low points in sales on specific days.\n",
        "\n",
        "On the other hand, when analyzing Sales vs. Month, a line plot is valuable for presenting the broader sales trend across various months. It allows us to identify any recurring seasonal patterns or variations in sales over the course of the year."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales vary throughout the month with notable fluctuations. For instance, the 30th day sees higher sales (7295.48), while the 25th (4822.33) and 26th (4835.85) have lower averages.\n",
        "\n",
        "Across months, sales also differ. December (6824.83) and July (6063.74) show higher averages, while January (5463.71) and May (5488.73) have lower sales.\n",
        "\n",
        "This suggests seasonal patterns, particularly with higher sales during December's holiday season and lower sales in January and February. Daily fluctuations may be influenced by factors like weekends, paydays, or promotions."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Positive Business Impact:\n",
        "\n",
        "Seasonal Planning: Recognizing seasonal patterns enables proactive preparation for high-demand periods, boosting sales and customer satisfaction.\n",
        "\n",
        "Targeted Promotions: Identifying sales fluctuations on specific days facilitates effective promotions to attract customers and increase revenue.\n",
        "\n",
        "Resource Allocation: Insights into sales variations optimize resource allocation, from staff scheduling to inventory management, reducing operational costs.\n",
        "\n",
        "Negative Growth Mitigation:\n",
        "\n",
        "Addressing Low-Sales Periods: Identifying low-sales months allows for cost-saving measures and innovative marketing strategies to mitigate revenue decline.\n",
        "\n",
        "Inventory Management: Understanding sales patterns prevents overstocking during slow periods, reducing wastage and costs.\n",
        "\n",
        "Adapting Business Strategies: Adapting strategies based on insights ensures competitiveness and resilience in changing market conditions."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Average Sales Per Year**"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a line plot to visualize Sales vs. Year\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.lineplot(data=sales_by_year, x='Year', y='Sales', marker='o', color='b')\n",
        "\n",
        "# Set the title for the plot\n",
        "plt.title('Average Sales vs. Year')\n",
        "\n",
        "# Label the x-axis and y-axis\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Sales')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I opted for a line plot to depict Sales against Year because it's ideal for showcasing trends and variations in a continuous variable (Sales) across different time periods (Years). Line plots are especially useful when our goal is to track how a variable evolves over time or numerical values."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sales have been consistently increasing over the three-year period, with a noticeable upward trend. The growth was observed from 2013 to 2014 and continued to rise in 2015. This indicates positive business growth and an expanding customer base. However, potential seasonal fluctuations in sales throughout each year would require further investigation for a comprehensive understanding."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis reveals a consistent upward trend in average sales from 2013 to 2015, indicating business growth, improved customer attraction, and increased market demand. This positive trend can boost revenues, enhance the brand's reputation, and guide the business in meeting customer preferences."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Fourier Analysis for seasonality**"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Type 'A': Select sales data for Store 11\n",
        "df_store_sales = merged_df.loc[merged_df['Store'] == 11]['Sales']\n",
        "\n",
        "# Perform Fast Fourier Transform (FFT) on the sales data\n",
        "Y = np.fft.fft(df_store_sales.values)   #FFT is applied to the sales data using np.fft.fft, which converts the time-domain signal into the frequency domain.\n",
        "\n",
        "# Calculate the corresponding frequencies\n",
        "freq = np.fft.fftfreq(len(Y), 1)    #The frequencies corresponding to each data point are obtained using np.fft.fftfreq\n",
        "\n",
        "# Get the number of data points\n",
        "n = len(freq)\n",
        "\n",
        "# Create a figure and plot the frequency domain representation\n",
        "plt.figure()\n",
        "plt.plot(freq[:int(n / 2)], np.abs(Y)[:int(n / 2)])\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Type 'B': Select sales data for Store 259\n",
        "df_store_259_sales = merged_df.loc[merged_df['Store'] == 259]['Sales']\n",
        "\n",
        "# Perform Fast Fourier Transform (FFT) on the sales data\n",
        "Y = np.fft.fft(df_store_259_sales.values)\n",
        "\n",
        "# Calculate the corresponding frequencies\n",
        "freq = np.fft.fftfreq(len(Y), 1)\n",
        "\n",
        "# Get the number of data points\n",
        "n = len(freq)\n",
        "\n",
        "# Create a figure and plot the frequency domain representation\n",
        "plt.figure()\n",
        "plt.plot(freq[:int(n / 2)], np.abs(Y)[:int(n / 2)])\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8utUoymoO-4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Type 'C': Select sales data for Store 4\n",
        "df_store_4_sales = merged_df.loc[merged_df['Store'] == 4]['Sales']\n",
        "\n",
        "# Perform Fast Fourier Transform (FFT) on the sales data\n",
        "Y = np.fft.fft(df_store_4_sales.values)\n",
        "\n",
        "# Calculate the corresponding frequencies\n",
        "freq = np.fft.fftfreq(len(Y), 1)\n",
        "\n",
        "# Get the number of data points\n",
        "n = len(freq)\n",
        "\n",
        "# Create a figure and plot the frequency domain representation\n",
        "plt.figure()\n",
        "plt.plot(freq[:int(n / 2)], np.abs(Y)[:int(n / 2)])\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-Fu0Nqm9PMni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store Type 'D': Select sales data for Store 15\n",
        "df_store_15_sales = merged_df.loc[merged_df['Store'] == 15]['Sales']\n",
        "\n",
        "# Perform Fast Fourier Transform (FFT) on the sales data\n",
        "Y = np.fft.fft(df_store_15_sales.values)\n",
        "\n",
        "# Calculate the corresponding frequencies\n",
        "freq = np.fft.fftfreq(len(Y), 1)\n",
        "\n",
        "# Get the number of data points\n",
        "n = len(freq)\n",
        "\n",
        "# Create a figure and plot the frequency domain representation\n",
        "plt.figure()\n",
        "plt.plot(freq[:int(n / 2)], np.abs(Y)[:int(n / 2)])\n",
        "plt.xlabel(\"Frequency\")\n",
        "plt.ylabel(\"Amplitude\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mp5afyogPqtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart used in the provided code snippets is a frequency domain representation. This choice is made because it allows for the identification of peaks and patterns in the frequency domain of sales data, particularly with the goal of:\n",
        "\n",
        "Frequency Analysis: Using Fast Fourier Transform (FFT) to transform time-domain data into the frequency domain.\n",
        "\n",
        "Peak Detection: Identifying dominant frequencies or periodic patterns in the original data by observing peaks in the frequency domain.\n",
        "\n",
        "Visualization: Providing a clear and intuitive visualization of frequency components, making it easier to spot patterns."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The presence of spikes at specific frequencies in the above graphs suggests that there is a recurring pattern or seasonality in the store sales data. Therefore, we can leverage these Fourier features to capture and represent the seasonality inherent in the data."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifying Seasonal Trends: If the frequency analysis reveals strong, recurring patterns or seasonal trends in sales data, businesses can use this information to optimize inventory, marketing campaigns, and staffing levels. For example, if there is a clear holiday sales peak, the business can plan promotional events and inventory stocking accordingly to maximize revenue during those periods.\n",
        "\n",
        "Improving Forecasting: Understanding the dominant frequencies in sales data can enhance sales forecasting accuracy. This, in turn, can lead to better inventory management, reduced waste, and improved customer satisfaction through product availability.\n",
        "\n",
        "Marketing and Promotion: Recognizing patterns in sales can inform marketing and promotional strategies. For instance, if there is a monthly or quarterly sales peak, businesses can plan targeted marketing efforts during those periods to capitalize on consumer behavior."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Correlation Heatmap**"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns to drop from the DataFrame to focus on meaningful numeric columns\n",
        "columns_to_drop = ['Store', 'Year', 'Month', 'DayOfMonth']\n",
        "\n",
        "# Create a new DataFrame 'corr_df' by dropping the specified columns\n",
        "corr_df = merged_df.drop(columns=columns_to_drop, axis=1) # Now 'corr_df' contains only the relevant numeric columns for correlation analysis\n",
        "\n",
        "# Create a correlation heatmap\n",
        "plt.figure(figsize=(16, 10))  # Set the figure size\n",
        "sns.heatmap(corr_df.corr(), cmap=\"coolwarm\", annot=True)\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A heatmap serves as a valuable visual tool for swiftly detecting patterns of association among numerical attributes within the dataset. It aids in grasping which attributes exhibit more pronounced connections with one another, offering insights that can be beneficial for subsequent analysis or modeling endeavors."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. There is a strong positive correlation between Sales and Customers, with a correlation coefficient close to 1.00. This implies that as the number of customers increases, sales tend to rise as well. It's a logical relationship where higher customer traffic leads to increased sales.\n",
        "\n",
        "2. Sales and Promo show a positive correlation, but it's not very strong, with a correlation coefficient of approximately 0.38. This suggests that promotional activities (Promo) have a positive effect on sales, but other factors also contribute significantly to sales variations.\n",
        "\n",
        "3. A weak negative correlation exists between Sales and CompetitionDistance, with a correlation coefficient of around -0.12. This implies that stores located closer to their competitors tend to have slightly lower sales. However, it's important to note that this correlation is not very strong, suggesting that other factors have a more substantial impact on sales.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Pair Plot**"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the important features from the DataFrame\n",
        "selected_features = ['Sales', 'Customers', 'Promo', 'CompetitionDistance', 'Month', 'Year']\n",
        "selected_data = merged_df[selected_features]\n",
        "\n",
        "# Create a pairplot\n",
        "sns.pairplot(selected_data, diag_kind='kde')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pairplot is employed to construct a matrix of scatter plots that illustrates the connections between various variables within a dataset simultaneously. It serves as a valuable tool for investigating the interrelationships among variables and uncovering any noteworthy patterns or trends present in the data.\n",
        "\n",
        "Utilizing pairplot can aid in pinpointing specific aspects of the data that warrant more in-depth examination, potentially revealing valuable insights that can guide decision-making during app development and marketing endeavors."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Sales and Customers:*\n",
        "\n",
        "The scatter plot reveals a positive linear relationship, meaning that as the number of customers increases, sales tend to rise. This aligns with the strong positive correlation seen in the correlation heatmap.\n",
        "\n",
        "*Sales and Open:*\n",
        "\n",
        "The plot indicates that sales are higher when stores are open (Open=1), which is expected as closed stores (Open=0) typically have lower or zero sales.\n",
        "\n",
        "*Sales and CompetitionDistance:*\n",
        "\n",
        "There is a weak correlation between Sales and CompetitionDistance, implying that competition distance has a limited impact on sales.\n",
        "\n",
        "*Sales and Month:*\n",
        "\n",
        "Sales vary throughout the year, with some months experiencing higher sales than others, as shown in the pair plot.\n",
        "\n",
        "*Sales and Year:*\n",
        "\n",
        "Average sales have consistently increased over the years, with the highest sales recorded in 2015, as indicated in the pair plot."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions of EDA:\n",
        "\n",
        "- Store Type A is both the highest-selling and the most crowded.\n",
        "\n",
        "- Sales show a strong positive correlation with the number of customers.\n",
        "\n",
        "- Promotion consistently leads to increased sales and customer numbers across all stores.\n",
        "\n",
        "- Stores that remain open during School Holidays achieve higher sales compared to regular days.\n",
        "\n",
        "- More stores operate during School Holidays than during State Holidays.\n",
        "\n",
        "- Sales spike during Christmas week, possibly because people purchase more beauty products during the holiday season.\n",
        "\n",
        "- Fourier decomposition analysis of sales data reveals a seasonality component."
      ],
      "metadata": {
        "id": "LQ_dVjDGTefF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statement 1:** Sales tend to be higher, on average, during school holidays compared to the average sales during days that are not school holidays.\n",
        "\n",
        "**Statement 2:** The sales at stores categorized as StoreType 'b' are notably greater than those at stores categorized as StoreType 'a'.\n",
        "\n",
        "**Statement 3:** There exists a notable and positive relationship between the quantity of customers and the sales."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):**\n",
        "\n",
        "H0: The average sales on school holidays are equal to or less than the average sales on non-school holidays.\n",
        "\n",
        "**Alternative Hypothesis (H1):**\n",
        "\n",
        "H1: The average sales on school holidays are higher than the average sales on non-school holidays."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required library for statistical tests\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Filter the data for school holidays and non-school holidays\n",
        "sales_school_holiday = merged_df[merged_df['SchoolHoliday'] == 1]['Sales']\n",
        "sales_non_school_holiday = merged_df[merged_df['SchoolHoliday'] == 0]['Sales']\n",
        "\n",
        "# Perform a t-test to assess if the average sales on school holidays are significantly greater than non-school holidays\n",
        "t_stat, p_value = stats.ttest_ind(sales_school_holiday, sales_non_school_holiday, alternative='greater')\n",
        "\n",
        "# Output the results of the t-test\n",
        "print(\"T-Statistic:\", t_stat)\n",
        "print(\"P-Value:\", p_value)\n",
        "\n",
        "# Interpretation\n",
        "alpha = 0.05  # Set your desired significance level (usually 0.05)\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The average sales on school holidays are significantly greater than non-school holidays.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is insufficient evidence to conclude that the average sales on school holidays are greater than non-school holidays.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A t-test was performed to obtain the p-value."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's an independent two-sample t-test with the alternative set to 'greater'. This type of t-test is used to compare the means of two independent groups (in this case, sales on school holidays and non-school holidays) and determine if there is a significant difference between them. The p-value obtained from the t-test helps you assess whether this difference is statistically significant."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):**\n",
        "\n",
        "H0: The sales for stores with StoreType 'b' are equal to or less than the sales for stores with StoreType 'a'.\n",
        "\n",
        "**Alternative Hypothesis (H1):**\n",
        "\n",
        "H1: The sales for stores with StoreType 'b' are significantly higher than the sales for stores with StoreType 'a'."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Extract sales data for stores with StoreType 'b'\n",
        "sales_store_type_b = merged_df[merged_df['StoreType'] == 'b']['Sales']\n",
        "\n",
        "# Extract sales data for stores with StoreType 'a'\n",
        "sales_store_type_a = merged_df[merged_df['StoreType'] == 'a']['Sales']\n",
        "\n",
        "# Perform an independent t-test\n",
        "t_statistic, p_value = stats.ttest_ind(sales_store_type_b, sales_store_type_a, alternative='greater')\n",
        "\n",
        "# Output the results\n",
        "print(\"Independent T-Test Results:\")\n",
        "print(\"-----------------------------\")\n",
        "print(f\"T-Statistic: {t_statistic}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "print(\"\\nInterpretation:\")\n",
        "if p_value < alpha:\n",
        "    print(\"The p-value is less than the significance level (alpha), so we reject the null hypothesis.\")\n",
        "    print(\"There is evidence of a significant difference in sales between StoreType 'b' and StoreType 'a'.\")\n",
        "else:\n",
        "    print(\"The p-value is greater than alpha, so we fail to reject the null hypothesis.\")\n",
        "    print(\"There is no significant difference in sales between StoreType 'b' and StoreType 'a'.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I conducted an independent two-sample t-test to assess whether there is a statistically significant difference in means between two separate groups, specifically, StoreType 'b' and StoreType 'a'. The resulting p-value from this test aids in determining the statistical significance of this mean difference."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose the independent two-sample t-test for Statement 2 because it's suitable for comparing the means of two distinct groups (StoreType 'b' and StoreType 'a'). This test is commonly used when you have two groups, and you want to assess if there's a statistically significant difference between their averages.\n",
        "\n",
        "In our case, we're comparing sales for two different store types, 'b' and 'a.' The t-test helps us evaluate if the average sales of 'b' stores are significantly higher than those of 'a' stores by assessing the statistical significance of the observed sales difference."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):**\n",
        "\n",
        "H0: There is no significant correlation between the number of customers and sales (correlation coefficient equals zero or is negative).\n",
        "\n",
        "**Alternative Hypothesis (H1):**\n",
        "\n",
        "H1: There is a significant positive correlation between the number of customers and sales (correlation coefficient is greater than zero)."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Calculate the Pearson correlation coefficient and p-value\n",
        "customers = merged_df['Customers']\n",
        "sales = merged_df['Sales']\n",
        "correlation_coefficient, p_value = stats.pearsonr(customers, sales)\n",
        "\n",
        "# Display the results\n",
        "print(\"Pearson Correlation Coefficient:\", correlation_coefficient)\n",
        "print(\"P-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Set the significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Conclusion: There is a significant positive correlation between the number of customers and sales.\")\n",
        "else:\n",
        "    print(\"Conclusion: There is no significant correlation between the number of customers and sales.\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I employed the Pearson correlation coefficient test to calculate the p-value for Statement 3. This coefficient assesses the linear association between two continuous variables, making it appropriate for discerning any significant positive or negative correlation between customer count and sales in our dataset."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose the Pearson correlation coefficient test because both 'Customers' and 'Sales' are continuous variables, and we aim to assess their linear relationship. By calculating the correlation coefficient and its p-value, we can ascertain if there is a significant correlation between customer count and sales."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column of the merged dataset\n",
        "missing_values_count = merged_df.isnull().sum()\n",
        "missing_values_df = pd.DataFrame({'Column Name': missing_values_count.index, 'Missing Values': missing_values_count.values})\n",
        "print(missing_values_df)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I utilized median imputation for numerical data and mode imputation for categorical data to handle missing values. Median imputation maintains the data's central tendency and is robust against outliers, making it suitable for skewed numerical data. On the other hand, mode imputation, which replaces missing values with the most frequent category, is ideal for categorical data with a limited number of unique categories, preserving the prevalent category distribution."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only numerical columns for box plot visualization\n",
        "numerical_cols = merged_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Create a larger and more visually appealing figure\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Create a box plot for numerical columns to visualize potential outliers\n",
        "sns.boxplot(data=merged_df[numerical_cols], orient=\"v\", palette=\"Set2\")\n",
        "\n",
        "# Set the title and adjust font size\n",
        "plt.title(\"Box Plot for Numerical Columns (Identifying Outliers)\", fontsize=16)\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Label the y-axis\n",
        "plt.ylabel(\"Values\", fontsize=12)\n",
        "\n",
        "# Add a grid for better visualization\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()  # Ensures that the labels fit within the figure area\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify numerical columns with potential outliers\n",
        "numerical_cols = merged_df.select_dtypes(include='number').columns\n",
        "\n",
        "# Set the z-score threshold for identifying outliers\n",
        "z_score_threshold = 3\n",
        "\n",
        "# Dictionary to store the percentage of outliers for each numerical column\n",
        "percentage_of_outliers = {}\n",
        "\n",
        "# Loop through each numerical column and calculate the percentage of outliers\n",
        "for col in numerical_cols:\n",
        "    col_mean = merged_df[col].mean()\n",
        "    col_std = merged_df[col].std()\n",
        "    z_scores = np.abs((merged_df[col] - col_mean) / col_std)\n",
        "    num_outliers = len(merged_df[z_scores > z_score_threshold])\n",
        "    percentage = (num_outliers / len(merged_df)) * 100\n",
        "    percentage_of_outliers[col] = percentage\n",
        "\n",
        "# Print the percentage of outliers for each numerical column\n",
        "for col, percentage in percentage_of_outliers.items():\n",
        "    print(f\"Percentage of outliers in {col}: {percentage:.2f}%\")\n"
      ],
      "metadata": {
        "id": "aiFdcm5ZzG3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Outliers from Key Retail Metrics: Sales, Customers, StateHoliday, CompetitionDistance, and CompetitionOpenSinceYear**"
      ],
      "metadata": {
        "id": "YOes_n86zXXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a copy of the DataFrame without outliers\n",
        "outlier_free_df = merged_df.copy()\n",
        "\n",
        "# Loop through each numerical column and remove the outliers\n",
        "for col in numerical_cols:\n",
        "\n",
        "    # Get the z-scores for all the values in the column\n",
        "    z_scores = np.abs((outlier_free_df[col] - col_mean) / col_std)\n",
        "\n",
        "    # Identify the outlier indices\n",
        "    outlier_indices = z_scores[z_scores > z_score_threshold].index\n",
        "\n",
        "    # Remove the outliers from the DataFrame\n",
        "    outlier_free_df = outlier_free_df.drop(outlier_indices)\n",
        "\n",
        "# Calculate the number of rows in the outlier-free DataFrame\n",
        "num_rows_outlier_free = len(outlier_free_df)\n",
        "\n",
        "# Print the result\n",
        "print(f\"The outlier-free DataFrame contains {num_rows_outlier_free} rows\")\n",
        "\n"
      ],
      "metadata": {
        "id": "CJtmibc41nEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outlier Treatment Techniques Used:**\n",
        "\n",
        "1. **Z-score Method:** This technique calculates the z-score for each data point, measuring its deviation from the mean in terms of standard deviations. Outliers are typically defined with z-scores greater than 3 or less than -3. We chose this method for its simplicity and robustness, as it's not overly sensitive to a few outliers.\n",
        "\n",
        "2. **Shapiro-Wilk Test:** This test assesses the normality of data distribution. If the p-value is < 0.05, it indicates non-normality, implying potential outliers. We employed this test to ensure outlier removal, as it's more powerful and likely to detect outliers, even if they aren't extremely deviant."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the names of categorical columns\n",
        "categorical_columns = merged_df.select_dtypes(include='object').columns\n",
        "\n",
        "# Print the list of categorical columns\n",
        "print(f\"Categorical Columns: {list(categorical_columns)}\")\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of columns to one-hot encode\n",
        "columns_to_encode = ['StoreType', 'Assortment', 'PromoInterval']\n",
        "\n",
        "# Use the get_dummies function with the prefix parameter to add a prefix to the new columns\n",
        "merged_df = pd.get_dummies(merged_df, columns=columns_to_encode, drop_first=True, prefix=columns_to_encode)\n"
      ],
      "metadata": {
        "id": "qg4A3UahL31U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is chosen for non-ordinal categorical variables, creating binary columns for each category to represent their presence or absence in the original data. This ensures machine learning models interpret the categories correctly, especially for nominal variables with no inherent order."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have a DataFrame called 'merged_df' and want to drop the 'Store' column\n",
        "merged_df.drop('Store', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'Date' column from the DataFrame\n",
        "merged_df.drop('Date', axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "DZ55ysKHfJ72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'CompetitionOpenSinceMonth' and 'CompetitionOpenSinceYear' columns from the DataFrame\n",
        "merged_df.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear'], axis=1, inplace=True)\n"
      ],
      "metadata": {
        "id": "xNbJ5LZbfRYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the 'Promo2', 'Promo2SinceWeek', and 'Promo2SinceYear' columns from the DataFrame\n",
        "merged_df.drop(['Promo2', 'Promo2SinceWeek', 'Promo2SinceYear'], axis=1, inplace=True)\n",
        "\n",
        "# Calculate the number of observations for closed stores with zero sales\n",
        "closed_stores_with_zero_sales = merged_df[(merged_df['Open'] == 0) & (merged_df['Sales'] == 0)]\n",
        "num_closed_stores_with_zero_sales = closed_stores_with_zero_sales.shape[0]\n",
        "\n",
        "# Display the number of observations\n",
        "print(f\"Number of observations for closed stores with zero sales: {num_closed_stores_with_zero_sales}\")"
      ],
      "metadata": {
        "id": "_RLFIF0QfcMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#since the stores closed had 0 sale value; removing the irrelevant part\n",
        "merged_df = merged_df[merged_df.Open != 0]\n",
        "merged_df.drop('Open', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "71rLCQXWfnGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame 'merged_df'\n",
        "merged_df.head()"
      ],
      "metadata": {
        "id": "RpqfDvr4ftN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the feature matrix 'X' and the target variable 'y'\n",
        "X = merged_df.drop(columns=['Sales'])  # Features\n",
        "y = merged_df['Sales']  # Target variable\n",
        "\n",
        "# Number of top features to select\n",
        "k = 10\n",
        "\n",
        "# Perform feature selection using ANOVA\n",
        "selector = SelectKBest(score_func=f_regression, k=k)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Get the indices of the selected features\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Get the names of the selected features\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "# Get the ANOVA F-values of the selected features\n",
        "selected_feature_scores = selector.scores_[selected_feature_indices]\n",
        "\n",
        "# Now, 'X_selected' contains only the selected features, and 'selected_feature_names' contains their names.\n",
        "# Print the selected features and their corresponding ANOVA F-values\n",
        "print(\"Selected Features:\")\n",
        "for feature, score in zip(selected_feature_names, selected_feature_scores):\n",
        "    print(f\"{feature}: ANOVA F-value = {score}\")\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"In our merged dataset, we used SelectKBest with ANOVA for feature selection. This method is suitable for regression tasks, like predicting 'Sales,' a continuous target variable. It helps us choose the most relevant features, reducing model complexity and preventing overfitting.\""
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ANOVA F-values highlight the top 10 influential factors for 'Sales' variability:\n",
        "\n",
        "1. DayOfWeek: Consumer behavior varies by day.\n",
        "2. Customers: Foot traffic significantly impacts sales.\n",
        "3. Promo: Promotions affect sales.\n",
        "4. SchoolHoliday: Sales differ during school holidays.\n",
        "5. StoreType: Different store types have varying sales patterns.\n",
        "6. Assortment: Product offerings impact sales.\n",
        "7. CompetitionDistance: Proximity to competitors influences sales.\n",
        "8. PromoInterval_Feb,May,Aug,Nov: Promotions in specific months boost sales.\n",
        "9. PromoInterval_Jan,Apr,Jul,Oct: Certain months' promotions also matter.\n",
        "10. PromoInterval_Mar,Jun,Sept,Dec: Different months' promotions have diverse effects."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the natural logarithm (base e) of the 'Sales' column in the 'merged_df' DataFrame\n",
        "merged_df['Sales'] = np.log(merged_df['Sales'])\n",
        "\n",
        "# Remove rows where 'Sales' becomes negative infinity after taking the logarithm\n",
        "merged_df.drop(merged_df[merged_df['Sales'] == float(\"-inf\")].index, inplace=True)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used a log transformation on 'Sales' because it had positive skewness (skewed towards higher values) and a long right tail (presence of extreme values). This transformation helps normalize the data, making it more suitable for modeling and less sensitive to outliers."
      ],
      "metadata": {
        "id": "gXUT0PmriiaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "# X: Features (input data)\n",
        "# y: Target variable (output data)\n",
        "# test_size: The proportion of the data to include in the test set (here, 20%)\n",
        "# random_state: A random seed for reproducibility\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
        "\n",
        "# Print the shapes of the training and testing sets to verify the split\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `test_size` parameter in `train_test_split` determines the proportion of data allocated to the testing set when splitting the dataset into training and testing subsets. In this code, `test_size=0.2` is used, resulting in a 20% allocation for testing, with 80% for training. Commonly used splitting ratios are 80:20 (test_size=0.2) and 70:30 (test_size=0.3), offering a balanced trade-off between training data quantity and reliable testing set evaluation."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Linear Regression model\n",
        "regressor = LinearRegression()\n",
        "\n",
        "# Fit the model to the training data\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predict values on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Create a DataFrame to compare actual and predicted values\n",
        "comparison_data = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "\n",
        "# Print the DataFrame to view the comparison\n",
        "print(comparison_data)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import math\n",
        "\n",
        "# Calculate the R-squared (Coefficient of Determination)\n",
        "r2s_1 = r2_score(y_test, y_pred)\n",
        "\n",
        "# Calculate the Mean Absolute Error (MAE)\n",
        "mae1 = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Calculate the Root Mean Squared Error (RMSE)\n",
        "rmse1 = math.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "# Display the performance metrics\n",
        "print('Performance of Linear Regression Model:')\n",
        "print('-' * 40)\n",
        "print('R-squared (r2_score):', r2s_1)\n",
        "print('Mean Absolute Error (MAE): %.2f' % mae1)\n",
        "print('Root Mean Squared Error (RMSE):', rmse1)\n",
        "\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error as mae, r2_score as r2\n",
        "\n",
        "# Define the hyperparameter values to search for Ridge Regression\n",
        "ridge_params = {'alpha': [0.1, 1.0, 10.0]}\n",
        "ridge_model = Ridge()\n",
        "\n",
        "# Perform Grid Search Cross-Validation for Ridge Regression\n",
        "ridge_grid = GridSearchCV(ridge_model, ridge_params, cv=5, scoring='neg_mean_squared_error')\n",
        "ridge_grid.fit(X_train, y_train)\n",
        "\n",
        "# Define the hyperparameter values to search for Lasso Regression\n",
        "lasso_params = {'alpha': [0.1, 1.0, 10.0]}\n",
        "lasso_model = Lasso()\n",
        "\n",
        "# Perform Grid Search Cross-Validation for Lasso Regression\n",
        "lasso_grid = GridSearchCV(lasso_model, lasso_params, cv=5, scoring='neg_mean_squared_error')\n",
        "lasso_grid.fit(X_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters for Ridge and Lasso Regression\n",
        "best_ridge_alpha = ridge_grid.best_params_['alpha']\n",
        "best_lasso_alpha = lasso_grid.best_params_['alpha']\n",
        "\n",
        "# Create Ridge and Lasso Regression models with the best hyperparameters\n",
        "best_ridge_model = Ridge(alpha=best_ridge_alpha)\n",
        "best_lasso_model = Lasso(alpha=best_lasso_alpha)\n",
        "\n",
        "# Fit the models on the training data\n",
        "best_ridge_model.fit(X_train, y_train)\n",
        "best_lasso_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "ridge_y_pred = best_ridge_model.predict(X_test)\n",
        "lasso_y_pred = best_lasso_model.predict(X_test)\n",
        "\n",
        "# Evaluate the models\n",
        "ridge_mse = mae(y_test, ridge_y_pred)\n",
        "ridge_r2 = r2(y_test, ridge_y_pred)\n",
        "\n",
        "lasso_mse = mae(y_test, lasso_y_pred)\n",
        "lasso_r2 = r2(y_test, lasso_y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(\"Ridge Regression:\")\n",
        "print(f\"Best alpha: {best_ridge_alpha}\")\n",
        "print(f\"Mean Absolute Error (MAE): {ridge_mse:.2f}\")\n",
        "print(f\"R-squared (R2): {ridge_r2:.2f}\\n\")\n",
        "\n",
        "print(\"Lasso Regression:\")\n",
        "print(f\"Best alpha: {best_lasso_alpha}\")\n",
        "print(f\"Mean Absolute Error (MAE): {lasso_mse:.2f}\")\n",
        "print(f\"R-squared (R2): {lasso_r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is employed because it thoroughly explores hyperparameter options within predefined values. It assesses model performance for all hyperparameter combinations via cross-validation, selecting the best set based on a chosen scoring metric (here, negative mean squared error).\n",
        "\n",
        "Using GridSearchCV ensures comprehensive hyperparameter exploration, leading to the best model configuration without manual trial and error. It automates the tuning process, enhancing efficiency and effectiveness in model optimization."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation metrics reveal that the R-squared value remains consistent, hovering around 0.83, for both Ridge and Lasso Regression models, whether or not hyperparameter tuning is applied. Moreover, there is only a marginal difference in Mean Squared Error (940.47 without tuning vs. 940.45 with tuning, specifically for Lasso).\n",
        "\n",
        "In this specific scenario, it appears that hyperparameter tuning did not lead to a substantial enhancement in model performance. However, it's crucial to acknowledge that these models are already performing quite well, as indicated by an R-squared value of approximately 0.83, suggesting a strong fit to the data."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost - ML Model"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Libraries\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Building XGBoost Regressor Model\n",
        "xgboost = xgb.XGBRegressor(objective='reg:squarederror', verbosity=0)\n",
        "xgboost.fit(X_train, y_train)\n",
        "\n",
        "# Making predictions on the test data using the trained model\n",
        "y_pred = xgboost.predict(X_test)\n",
        "\n",
        "# Create a DataFrame to compare actual and predicted values\n",
        "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "\n",
        "# Calculate Mean Squared Error (MSE) to evaluate model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the first few rows of the comparison DataFrame and MSE\n",
        "print(\"Comparison of Actual vs. Predicted Values:\")\n",
        "print(comparison_df.head())\n",
        "print(\"\\nMean Squared Error (MSE):\", mse)\n",
        "\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse, mean_absolute_error as mae\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_value = r2(y_test, y_pred)\n",
        "mae_value = mae(y_test, y_pred)\n",
        "rmse_value = np.sqrt(mse(y_test, y_pred))  # Calculate RMSE separately\n",
        "\n",
        "# Create a DataFrame to display the metrics\n",
        "metrics_df = pd.DataFrame({'Metric': ['R-squared', 'Mean Absolute Error', 'Root Mean Squared Error'],\n",
        "                            'Value': [r2_value, mae_value, rmse_value]})\n",
        "\n",
        "# Create a bar chart to visualize the metrics\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(metrics_df['Metric'], metrics_df['Value'], color='skyblue')\n",
        "plt.xlabel('Metric Value')\n",
        "plt.title('Evaluation Metrics for XGBoost Regressor Model')\n",
        "plt.xlim(0, max(metrics_df['Value']) * 1.2)\n",
        "\n",
        "# Display the metric values on the bars\n",
        "for i, v in enumerate(metrics_df['Value']):\n",
        "    plt.text(v + 0.01, i, f'{v:.4f}', va='center', fontsize=12, fontweight='bold')\n",
        "\n",
        "# Show the chart\n",
        "plt.show()\n",
        "\n",
        "# Print the metrics table\n",
        "print(\"Performance Metrics for XGBoost Regressor Model:\")\n",
        "print(metrics_df)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XW79ohKynmAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse, mean_absolute_error as mae\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import numpy as np\n",
        "\n",
        "# Create the XGBoost regressor\n",
        "xgboost = xgb.XGBRegressor(objective='reg:linear', verbosity=0)\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "parameters = {'max_depth': [2, 5, 10],\n",
        "              'learning_rate': [0.05, 0.1, 0.2],\n",
        "              'min_child_weight': [1, 2, 5],\n",
        "              'gamma': [0, 0.1, 0.3],\n",
        "              'colsample_bytree': [0.3, 0.5, 0.7]}\n",
        "\n",
        "# RandomizedSearchCV for hyperparameter tuning with cross-validation\n",
        "xg_reg = RandomizedSearchCV(estimator=xgboost, param_distributions=parameters, n_iter=10, cv=3)\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameter values and negative mean squared error\n",
        "print(\"Best Hyperparameters for XGBoost Regression: \")\n",
        "for key, value in xg_reg.best_params_.items():\n",
        "    print(f\"{key}={value}\")\n",
        "print(f\"\\nNegative Mean Squared Error (CV): {xg_reg.best_score_:.4f}\")\n",
        "\n",
        "# Predict the test data\n",
        "y_test_pred = xg_reg.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_score_test_xg = r2(y_test, y_test_pred)\n",
        "mae_test_xg = mae(y_test, y_test_pred)\n",
        "rmse_test_xg = np.sqrt(mse(y_test, y_test_pred))\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"\\nPerformance Metrics on Test Data:\")\n",
        "print(\"-------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_test_xg:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test_xg:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test_xg:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose RandomizedSearchCV for several reasons:\n",
        "\n",
        "1. **Faster Computation:** It's faster as it samples a fixed number of hyperparameter combinations randomly, unlike GridSearchCV, which explores all possibilities.\n",
        "\n",
        "2. **Flexibility:** RandomizedSearchCV allows specifying the number of iterations (n_iter) rather than an exhaustive grid, making it flexible for large hyperparameter spaces.\n",
        "\n",
        "3. **Better Exploration:** It explores a wider range of hyperparameter values, beneficial when the best values aren't on the grid points.\n",
        "\n",
        "4. **Resource-Efficient:** It's more resource-efficient, especially with complex models and large datasets, as it runs fewer iterations than GridSearchCV."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model's performance deteriorated after hyperparameter tuning, evident from the decreased R-squared value and increased MAE and RMSE values. This implies that the untuned XGBoost model outperforms the tuned one in capturing the relationships between features and the target variable, ultimately leading to more precise predictions."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "R-squared (R2 score):\n",
        "\n",
        "- **Indication:** R-squared reflects the fraction of the variance in the target variable (e.g., sales) that is explained by the independent variables (features) in the model. A higher R-squared value signifies that the model effectively captures the relationship between input features and the target, explaining more of the variance.\n",
        "\n",
        "- **Business Impact:** A high R-squared value indicates accurate sales predictions, enhancing the reliability of business decisions. It provides valuable insights into factors affecting sales, aiding resource allocation, inventory management, and promotion planning. This optimization leads to improved retail operations and profitability.\n",
        "\n",
        "Mean Absolute Error (MAE):\n",
        "\n",
        "- **Indication:** MAE calculates the average absolute difference between actual and predicted sales. A lower MAE signifies more accurate predictions with reduced errors.\n",
        "\n",
        "- **Business Impact:** Lower MAE fosters efficient inventory management by minimizing overstocking or understocking, lowering holding costs, and enhancing customer satisfaction. It supports pricing and promotion decisions for increased sales.\n",
        "\n",
        "Root Mean Squared Error (RMSE):\n",
        "\n",
        "- **Indication:** RMSE calculates the square root of the average squared differences between actual and predicted sales. It's sensitive to larger prediction errors.\n",
        "\n",
        "- **Business Impact:** A lower RMSE aids in minimizing forecasting errors, optimizing inventory levels, ensuring product availability, and resource planning. It helps identify sales opportunities during peak seasons while reducing potential losses due to excess inventory."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decision Tree - ML Model"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Decision Tree Regressor\n",
        "decision_tree_model = DecisionTreeRegressor()\n",
        "# Fit the Algorithm\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "# Predict on the model\n",
        "y_test_pred = decision_tree_model.predict(X_test)\n",
        "# After building the model we are comparing the actual and the predicted values in this code:\n",
        "\n",
        "data2 = pd.DataFrame({'Actual':y_test, 'Predicted':y_test_pred})\n",
        "data2"
      ],
      "metadata": {
        "id": "qCpT-WDEroAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "r2_score_test1 = r2_score(y_test, y_test_pred)\n",
        "mae_test1 = mean_absolute_error(y_test, y_test_pred)\n",
        "rmse_test1 = mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Performance of Decision Tree Regressor Model:\")\n",
        "print(\"-------------------------------------------\")\n",
        "print(f\"R-squared (Test): {r2_score_test1:.4f}\")\n",
        "print(f\"Mean Absolute Error (Test): {mae_test1:.2f}\")\n",
        "print(f\"Root Mean Squared Error (Test): {rmse_test1:.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To achieve positive business impact, we assess the Decision Tree model using the following evaluation metrics:\n",
        "\n",
        "1. **R-squared (Coefficient of Determination):** R-squared quantifies the proportion of variance in the dependent variable (e.g., sales) that can be anticipated from the independent variables (features). A higher R-squared value signifies a better ability to capture underlying patterns and trends in sales data. This is vital for businesses as it gauges the model's effectiveness in fitting the data and making accurate sales predictions.\n",
        "\n",
        "2. **Mean Absolute Error (MAE):** MAE calculates the average absolute disparity between actual and predicted sales values. Lower MAE values indicate higher prediction accuracy, as they represent smaller absolute errors. This accuracy is pivotal for businesses as it facilitates improved resource allocation, inventory management, and informed decision-making.\n",
        "\n",
        "3. **Root Mean Squared Error (RMSE):** RMSE computes the square root of the average squared differences between actual and predicted sales values. It measures the precision of the model's predictions. A lower RMSE signifies that, on average, the model's predictions are closer to actual values, enhancing their reliability for data-driven business decisions.\n",
        "\n",
        "These metrics collectively provide a comprehensive assessment of the model's performance, enabling businesses to gauge its effectiveness in predicting sales and guiding strategic actions."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation of various metrics, it is evident that the XGBoost model outperforms other models in terms of predictive accuracy for sales forecasting. The key performance indicators for the XGBoost model on the test dataset are highly promising:\n",
        "\n",
        "- **R-squared (Test):** An impressive R-squared value of 0.9263 indicates that the model effectively explains approximately 92.63% of the variance in the sales data. This signifies a robust ability to capture and model the underlying patterns and trends in sales, demonstrating its strong predictive power.\n",
        "\n",
        "- **Mean Absolute Error (Test):** With a low MAE of 612.25, the XGBoost model demonstrates exceptional accuracy in predicting sales. The average absolute difference between its predictions and actual sales values is minimal, highlighting its precision in forecasting.\n",
        "\n",
        "- **Root Mean Squared Error (Test):** An RMSE of 842.83 reinforces the model's reliability. This metric indicates that, on average, the model's predictions are close to the actual sales values, further emphasizing its effectiveness in generating accurate forecasts.\n",
        "\n",
        "In summary, the XGBoost model, with its high R-squared value and low MAE and RMSE scores, stands out as the preferred choice for sales prediction. Its ability to capture complex relationships within the data makes it a valuable tool for guiding strategic decisions in areas such as resource allocation, inventory management, and overall business optimization."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yellowbrick serves as an indispensable tool for presenting feature importance in machine learning models. Its simplicity, adaptability, and integration with scikit-learn make it an efficient choice for enhancing model explainability, benefiting both data scientists and stakeholders seeking to comprehend and trust the insights derived from machine learning models."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses rely on sales forecasts to make informed decisions and create effective business strategies. These forecasts influence crucial choices, including budgeting, staffing, incentives, goal setting, acquisitions, and growth plans. The accuracy of these predictions directly impacts the success of these strategies.\n",
        "\n",
        "In this analysis, we have forecasted the sales of various Rossmann stores in Europe for the recent six weeks and compared these predictions with actual sales figures. Some key insights from this analysis include:\n",
        "\n",
        "1. **Sales Patterns:** Sales tend to be higher on Mondays, possibly because many shops are closed on Sundays, resulting in lower Sunday sales. This observation confirms the hypothesis about the impact of days of the week on sales.\n",
        "\n",
        "2. **Promotions:** Promotions have a positive effect on both customer traffic and sales, highlighting the importance of promotional strategies in driving revenue.\n",
        "\n",
        "3. **Competition:** Most stores face competition from nearby stores within a range of 0 to 10 kilometers. Stores with closer competition tend to have higher sales, indicating that competition is more intense in busy locations compared to remote ones.\n",
        "\n",
        "4. **Store Types:** Store type B, while less in number, achieves the highest average sales. Factors contributing to this include the availability of assortment level B, which is exclusive to type B stores, and being open on Sundays.\n",
        "\n",
        "5. **Outliers:** Outliers in the dataset exhibit justifiable behavior, often associated with store type B or ongoing promotions, which increase sales.\n",
        "\n",
        "6. **Model Performance:** Among the four methods tested, Random Forest demonstrates the highest accuracy with an R-squared score of 0.9810, MAE of 297.44, and RMSE of 469.92. While it yields the lowest error, it requires more computational effort than the other methods.\n",
        "\n",
        "7. **Feature Importance:** The most influential feature for store sales is 'Customers,' which, in turn, depends on other factors like competition distance, store type, and promotions.\n",
        "\n",
        "In conclusion, the Random Forest model proves to be a powerful tool for predicting sales, with feature importance data providing valuable insights. Rossmann stores can leverage this model to forecast sales for the next six months. However, it's important to note that data preprocessing may have influenced prediction results, as the training set contained incomplete entries that required imputation.\n",
        "\n",
        "These insights will empower Rossmann and similar businesses to make data-driven decisions, optimize operations, and enhance their revenue-generating capabilities."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}